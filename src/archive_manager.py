#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  - å‡¦ç†æ¸ˆã¿è«–æ–‡ã®åœ§ç¸®ä¿å­˜
"""
import json
import gzip
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

class ArchiveManager:
    """å‡¦ç†æ¸ˆã¿è«–æ–‡ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç®¡ç†"""
    
    def __init__(self, archive_dir: str = "data/archive"):
        self.archive_dir = archive_dir
        self.daily_archive_limit = 50  # 1æ—¥ã‚ãŸã‚Šã®æœ€å¤§ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä»¶æ•°
        os.makedirs(archive_dir, exist_ok=True)
    
    def archive_processed_articles(self, articles: List[Dict[str, Any]]) -> int:
        """å‡¦ç†æ¸ˆã¿è¨˜äº‹ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«ä¿å­˜"""
        if not articles:
            return 0
        
        # æ—¥ä»˜åˆ¥ã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        today = datetime.now().strftime("%Y%m%d")
        archive_file = os.path.join(self.archive_dir, f"processed_{today}.jsonl.gz")
        
        archived_count = 0
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        archive_data = []
        for article in articles:
            # å¿…è¦ãªæƒ…å ±ã®ã¿ã‚’æŠ½å‡ºã—ã¦ã‚µã‚¤ã‚ºå‰Šæ¸›
            archived_article = {
                "id": article.get("id"),
                "title": article.get("title"),
                "journal": article.get("journal"),
                "authors": article.get("authors", [])[:3],  # æœ€å¤§3åã¾ã§
                "summary_ja": article.get("summary_ja", "")[:200],  # 200æ–‡å­—ã¾ã§
                "published": article.get("published"),
                "processed_at": datetime.now().isoformat(),
                "priority": article.get("priority"),
                "link": article.get("link")
            }
            archive_data.append(archived_article)
        
        # åœ§ç¸®ã—ã¦ä¿å­˜
        try:
            with gzip.open(archive_file, 'at', encoding='utf-8') as f:
                for article in archive_data:
                    f.write(json.dumps(article, ensure_ascii=False) + '\\n')
                    archived_count += 1
            
            print(f"ğŸ“¦ Archived {archived_count} processed articles to {archive_file}")
            return archived_count
            
        except Exception as e:
            print(f"Error archiving articles: {e}")
            return 0
    
    def get_archive_stats(self, days: int = 30) -> Dict[str, Any]:
        """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        stats = {
            "total_files": 0,
            "total_articles": 0,
            "size_mb": 0.0,
            "date_range": [],
            "files": []
        }
        
        if not os.path.exists(self.archive_dir):
            return stats
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
        for filename in os.listdir(self.archive_dir):
            if filename.endswith('.jsonl.gz'):
                filepath = os.path.join(self.archive_dir, filename)
                file_stats = os.stat(filepath)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’åé›†
                file_info = {
                    "filename": filename,
                    "size_mb": file_stats.st_size / (1024 * 1024),
                    "modified": datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
                    "article_count": self._count_articles_in_archive(filepath)
                }
                
                stats["files"].append(file_info)
                stats["total_files"] += 1
                stats["total_articles"] += file_info["article_count"]
                stats["size_mb"] += file_info["size_mb"]
        
        # æ—¥ä»˜ç¯„å›²ã‚’è¨­å®š
        if stats["files"]:
            dates = [f["modified"][:10] for f in stats["files"]]
            stats["date_range"] = [min(dates), max(dates)]
        
        return stats
    
    def _count_articles_in_archive(self, filepath: str) -> int:
        """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®è¨˜äº‹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        try:
            with gzip.open(filepath, 'rt', encoding='utf-8') as f:
                return sum(1 for line in f if line.strip())
        except Exception:
            return 0
    
    def search_archive(self, query: str, days: int = 30) -> List[Dict[str, Any]]:
        """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰è¨˜äº‹ã‚’æ¤œç´¢"""
        results = []
        query_lower = query.lower()
        
        # æœ€è¿‘ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        cutoff_date = datetime.now() - timedelta(days=days)
        
        for filename in os.listdir(self.archive_dir):
            if not filename.endswith('.jsonl.gz'):
                continue
                
            filepath = os.path.join(self.archive_dir, filename)
            file_stats = os.stat(filepath)
            
            # æŒ‡å®šæœŸé–“å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿æ¤œç´¢
            if datetime.fromtimestamp(file_stats.st_mtime) < cutoff_date:
                continue
            
            try:
                with gzip.open(filepath, 'rt', encoding='utf-8') as f:
                    for line in f:
                        if not line.strip():
                            continue
                        
                        try:
                            article = json.loads(line)
                            
                            # ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯è¦ç´„ã«æ¤œç´¢ã‚¯ã‚¨ãƒªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                            title = article.get('title', '').lower()
                            summary = article.get('summary_ja', '').lower()
                            
                            if query_lower in title or query_lower in summary:
                                results.append(article)
                                
                        except json.JSONDecodeError:
                            continue
                            
            except Exception as e:
                print(f"Error searching archive {filename}: {e}")
                continue
        
        return results
    
    def cleanup_old_archives(self, keep_days: int = 90) -> int:
        """å¤ã„ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
        if not os.path.exists(self.archive_dir):
            return 0
        
        cutoff_date = datetime.now() - timedelta(days=keep_days)
        removed_count = 0
        
        for filename in os.listdir(self.archive_dir):
            if not filename.endswith('.jsonl.gz'):
                continue
                
            filepath = os.path.join(self.archive_dir, filename)
            file_stats = os.stat(filepath)
            
            if datetime.fromtimestamp(file_stats.st_mtime) < cutoff_date:
                try:
                    os.remove(filepath)
                    removed_count += 1
                    print(f"ğŸ—‘ï¸  Removed old archive: {filename}")
                except Exception as e:
                    print(f"Error removing {filename}: {e}")
        
        return removed_count
    
    def export_monthly_summary(self, year: int, month: int) -> Optional[str]:
        """æœˆæ¬¡ã‚µãƒãƒªãƒ¼ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        month_str = f"{year:04d}{month:02d}"
        summary_file = os.path.join(self.archive_dir, f"summary_{month_str}.json")
        
        monthly_stats = {
            "year": year,
            "month": month,
            "total_articles": 0,
            "journals": {},
            "top_keywords": {},
            "generated_at": datetime.now().isoformat()
        }
        
        # ãã®æœˆã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        for filename in os.listdir(self.archive_dir):
            if not filename.startswith(f"processed_{month_str}") or not filename.endswith('.jsonl.gz'):
                continue
            
            filepath = os.path.join(self.archive_dir, filename)
            
            try:
                with gzip.open(filepath, 'rt', encoding='utf-8') as f:
                    for line in f:
                        if not line.strip():
                            continue
                        
                        try:
                            article = json.loads(line)
                            monthly_stats["total_articles"] += 1
                            
                            # ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«çµ±è¨ˆ
                            journal = article.get('journal', 'Unknown')
                            monthly_stats["journals"][journal] = monthly_stats["journals"].get(journal, 0) + 1
                            
                            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰
                            title_words = article.get('title', '').lower().split()
                            for word in title_words:
                                if len(word) > 4:  # 4æ–‡å­—ä»¥ä¸Šã®å˜èªã®ã¿
                                    monthly_stats["top_keywords"][word] = monthly_stats["top_keywords"].get(word, 0) + 1
                                    
                        except json.JSONDecodeError:
                            continue
                            
            except Exception as e:
                print(f"Error processing {filename} for summary: {e}")
                continue
        
        # ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        try:
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(monthly_stats, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“Š Monthly summary exported: {summary_file}")
            return summary_file
            
        except Exception as e:
            print(f"Error exporting monthly summary: {e}")
            return None